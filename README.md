# LLaMA 2 Chatbot App âš¡

[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/gregwdata/quack_to_my_data?quickstart=1)

## ðŸ¤” What is this?

This is a project meant to get hands-on with the concept of using an LLM to engage with a database.

It will use a local instance of ðŸ¦† DuckDB to store some sample data sets. It will serve as a vehicle for exploring different approaches to guide the AI to produce correct and useful queries against the dataset.

Development will initially be centered around running in GitHub Codespaces

## ðŸ¤¨ Why do this?

After hearing so many discussions of and articles about LLMs where everyone points to a "talk-to-your-database" use case as some kind of self-evidently worthwhile example of what LLMs can do, it seemed worthwhile to learn more about it. 

How easy is it to implement yourself?

Does it actually benefit a non-technical user? I.E. do the questions input to the LLM need to be expressed in a way that maps one-to-one with SQL? ("What is the total number of sales I made to each customer in the midwest in the second quarter?") If you know SQL, it might be natural to state your question that way, but then you don't need an LLM getting between you and your data. What if a user asks "How were our numbers in the midwest market last quarter?" or tries to actually derive an *actionable* insight directly: "What machine is the worst-performing bottleneck in my factory?"

How well will a general-purpose model perform? Or will you need a model specifically trained on SQL tasks, like [NSQL](https://github.com/NumbersStationAI/NSQL) to get usable queries?

What are the limits of complexity that can be achieved? How many joins before the model can't keep up?

And is there any real **value** to something like this, other than being a novelty? Would it serve a useful purpose in fielding initial inquiries from non-SQL-fluent stakeholder? Can it help a technical analyst perform more efficiently?

The catalyst for action was listening to a couple podcast episodes in close succession: 

One was [Maxime Beauchemin on the MLOps Community Podcast](https://home.mlops.community/home/videos/treating-prompt-engineering-more-like-code) describe the approach to working with LLMs in a very clear and systematic way. Listening to him break down the process of approaching the development of LLM-based applications made it seem actually *doable*. I had a sense of where I could start and what to try.

The other was an episode of [Latent Space / The AI Breakdown](https://www.latent.space/p/breakdown), which included a duscussion of just how far along the Llama2 model was in capability. With a commercial-friendly license on such a powerful model, that one could run on ones own infrastructure, is there line of sight to using a model like Llama2 for real internal enterprise usage?

It sure seemed like a good time to take it for a spin and find out.

I'm hoping to build this in a way that serves as an example of "here's a way you could do it" (not here's how you *should* do it, of course) for anyone else curious about this in the same way I was. I want it to be accessible and easy for someone else to pick up and experiment with.

### ðŸ¦† Why use DuckDB?

In a word: simplicity. In another word: performance.

I wanted to strip away the particulars of dealing with the database - Python connection APIs are available for nearly any database one might want to apply this approach to, so it's somewhat orthogonal to the purpose of this project which database is used.

DuckDB has the nice property that it runs inside the Python process the of the rest of the app, removing deployment and development headaches around managing a database instance.

For the purpose of exercising this LLM-SQL concept with a variety of different datasets and database structures, DuckDB makes it trivially simple to switch datasets on the fly by connecting to a different `.duckdb` file. Connection to remote-hosted files and cloud data stores is also easy.

And storage is efficient enough that I can include several `.duckdb` database files within this GitHub repository with enough data to make them interesting to work with, and not worry about file size.

Its SQL dialect inlcudes the vast majority of commands an LLM is likely to come up with while building queries. Both ANSI SQL concepts and convenience methods extending SQL used by many common databases work with DuckDB.

DuckDB is extremely fast for every analytics-focused use case where I've used it. Using it here would minimize latency in the iteration of LLM to database - we're spending enough time waiting for LLM API calls to return! Plus, its performance may help to compensate for poorly-optimized or unusual queries generated by the LLM.

## Features

## Usage on Codespaces


## Authors

## Version

0.0.1 (initial setup) August 2023

## Contributing

This project is under development. Contributions are welcome!

## License

- This repo was started from https://github.com/a16z-infra/llama2-chatbot, which is published with an Apache 2.0 License
- Modifications made as part of derivative work in this project are licensed MIT

## Disclaimer

This is an experimental version of the app. Use at your own risk. While the app has been tested, the authors hold no liability for any kind of losses arising out of using this application. 

## Resources

- [Streamlit Cheat Sheet](https://docs.streamlit.io/library/cheatsheet)
- [GitHub to deploy LLaMA2 on Replicate](https://github.com/a16z-infra/cog-llama-template)
