# LLaMA 2 Chatbot App ‚ö°

[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/gregwdata/quack_to_my_data?quickstart=1)

## ü§î What is this?

This is a project meant to get hands-on with the concept of using an LLM to engage with a database.

It will use a local instance of ü¶Ü DuckDB to store some sample data sets. It will serve as a vehicle for exploring different approaches to guide the AI to produce correct and useful queries against the dataset.

Development will initially be centered around running in GitHub Codespaces

## ü§® Why do this?

After hearing so many discussions of and articles about LLMs where everyone points to a "talk-to-your-database" use case as some kind of self-evidently worthwhile example of what LLMs can do, it seemed worthwhile to learn more about it. 

How easy is it to implement yourself?

Does it actually benefit a non-technical user? I.E. do the questions input to the LLM need to be expressed in a way that maps one-to-one with SQL? ("What is the total number of sales I made to each customer in the midwest in the second quarter?") If you know SQL, it might be natural to state your question that way, but then you don't need an LLM getting between you and your data. What if a user asks "How were our numbers in the midwest market last quarter?" or tries to actually derive an *actionable* insight directly: "What machine is the worst-performing bottleneck in my factory?"

How well will a general-purpose model perform? Or will you need a model specifically trained on SQL tasks, like [NSQL](https://github.com/NumbersStationAI/NSQL) to get usable queries?

What are the limits of complexity that can be achieved? How many joins before the model can't keep up?

And is there any real **value** to something like this, other than being a novelty? Would it serve a useful purpose in fielding initial inquiries from non-SQL-fluent stakeholder? Can it help a technical analyst perform more efficiently?

The catalyst for action was listening to a couple podcast episodes in close succession: 

One was [Maxime Beauchemin on the MLOps Community Podcast](https://home.mlops.community/home/videos/treating-prompt-engineering-more-like-code) describe the approach to working with LLMs in a very clear and systematic way. Listening to him break down the process of approaching the development of LLM-based applications made it seem actually *doable*. I had a sense of where I could start and what to try.

The other was an episode of [Latent Space / The AI Breakdown](https://www.latent.space/p/breakdown), which included a duscussion of just how far along the Llama2 model was in capability. With a commercial-friendly license on such a powerful model, that one could run on ones own infrastructure, is there line of sight to using a model like Llama2 for real internal enterprise usage?

It sure seemed like a good time to take it for a spin and find out.

I'm hoping to build this in a way that serves as an example of "here's a way you could do it" (not here's how you *should* do it, of course) for anyone else curious about this in the same way I was. I want it to be accessible and easy for someone else to pick up and experiment with.

### ü¶Ü Why use DuckDB?

In a word: simplicity. In another word: performance.

I wanted to strip away the particulars of dealing with the database - Python connection APIs are available for nearly any database one might want to apply this approach to, so it's somewhat orthogonal to the purpose of this project which database is used.

DuckDB has the nice property that it runs inside the Python process the of the rest of the app, removing deployment and development headaches around managing a database instance.

For the purpose of exercising this LLM-SQL concept with a variety of different datasets and database structures, DuckDB makes it trivially simple to switch datasets on the fly by connecting to a different `.duckdb` file. Connection to remote-hosted files and cloud data stores is also easy.

And storage is efficient enough that I can include several `.duckdb` database files within this GitHub repository with enough data to make them interesting to work with, and not worry about file size.

Its SQL dialect inlcudes the vast majority of commands an LLM is likely to come up with while building queries. Both ANSI SQL concepts and convenience methods extending SQL used by many common databases work with DuckDB.

DuckDB is extremely fast for every analytics-focused use case where I've used it. Using it here would minimize latency in the iteration of LLM to database - we're spending enough time waiting for LLM API calls to return! Plus, its performance may help to compensate for poorly-optimized or unusual queries generated by the LLM.

## Features

## Datasets

To add a new dataset:

* If needed, write a helper script in `./db_utils/`. The script should write the db as a `.duckdb` file to a subfolder of the `./db_files/` directory.

    * The subfolder should have a name in common with the database file

* Add a section to the `Makefile` to create the subfolder and run the script. If no script needed (data already prepared)

* In `db_specific_prompts` add an item to the dictionary matching the database name

### TPC-H 

A [TPC-H](https://www.tpc.org/tpch/) benchmark dataset is created using DuckDB's `tpch` extension. For now the scale factor used is `0.1` for speed of creation and keeping the database to a reasonable small size for development.

## Usage on Codespaces
Start Codespaces on this repository by clicking [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/gregwdata/quack_to_my_data?quickstart=1)
, or you can click on the Green code button on top right of the repo.

To run this app, you will need to have a [replicate](https://replicate.com/) API key. Once you've obtained it, there are three options for using it within the app in Codespaces:

* Following [these instructions](https://docs.github.com/en/codespaces/managing-your-codespaces/managing-encrypted-secrets-for-your-codespaces), you can securely store your API key as a secret within GitHub, tied to this repository. Be sure to name it `REPLICATE_API_TOKEN` so it is available in the codespace as an environment variable with the correct name.
    * Using this method, if you set up this secret before opening the repo in Codespaces, the App will work correctly right out of the box.
* In the `.env` file, uncomment the `REPLICATE_API_TOKEN` line, and add your API token there. `.env` is inlcuded in the `.gitignore` file, so this key will not be committed.
    * You will need to restart the app using `make run_app` after adding this the first time you start your Codepsace
* Manually add it as an environment variable with `export REPLICATE_API_TOKEN=<your token here>`
    * You will need to restart the app using `make run_app` after adding this the first time you start your Codepsace


When the app runs on startup or manually by running `make run_app` at the command line, the app should load within a preview browser inside the IDE. You can also click the `Ports` tab in the lower pane and the üåê icon that appears when you hover over the local address field to open it in its own browser tab.

## Authors

## Version

0.0.1 (initial setup) August 2023

## Contributing

This project is under development. Contributions are welcome!

## License

- This repo was started from https://github.com/a16z-infra/llama2-chatbot, which is published with an Apache 2.0 License
- Modifications made as part of derivative work in this project are licensed MIT

## Disclaimer

This is an experimental version of the app. Use at your own risk. While the app has been tested, the authors hold no liability for any kind of losses arising out of using this application. 

## Resources

- [Streamlit Cheat Sheet](https://docs.streamlit.io/library/cheatsheet)
- [GitHub to deploy LLaMA2 on Replicate](https://github.com/a16z-infra/cog-llama-template)
